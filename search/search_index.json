{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-the-ml-engineers-blog","title":"Welcome the ML Engineer's Blog","text":""},{"location":"#about-me","title":"About me:","text":"I'm a Machine Learning Engineer. My work encompasses the design of machine learning algorithms and natural language processing (NLP) in a variety of contexts, from industry to academic research. Specialized in Artificial Intelligence, I also hold a Master's degree in Cognitive Sciences and more specifically in Cognitive Dynamics.   <p>My CV</p> <p> </p> Tools Language Environments IoT Machine Learning &amp; AI Libraries Scientific Writing &amp; Documentation <p> </p>"},{"location":"#i-worked-for","title":"<p> I worked for </p>","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/08/05/chatbot-memory/","title":"ChatBot Memory","text":"<p>Nous pr\u00e9sentons ici une approche pour la gestion de la m\u00e9moire \u00e0 court terme dans les chatbots, en utilisant une combinaison de techniques de stockage et de r\u00e9sum\u00e9 automatique pour optimiser le contexte conversationnel. La m\u00e9thode introduite repose sur une structure de m\u00e9moire dynamique qui limite la taille des donn\u00e9es tout en pr\u00e9servant les informations essentielles \u00e0 travers des r\u00e9sum\u00e9s intelligents. Cette approche permet non seulement d'am\u00e9liorer la fluidit\u00e9 des interactions mais aussi d'assurer une continuit\u00e9 contextuelle lors de longues sessions de dialogue. En outre, l'utilisation de techniques asynchrones garantit que les op\u00e9rations de gestion de la m\u00e9moire n'interf\u00e8rent pas avec la r\u00e9activit\u00e9 du chatbot.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#modelisation-mathematique-de-la-gestion-des-conversations","title":"Mod\u00e9lisation Math\u00e9matique de la Gestion des Conversations","text":"<p>Dans cette section, nous formalisons math\u00e9matiquement la gestion de la m\u00e9moire de conversation dans le chatbot. La m\u00e9moire est structur\u00e9e comme une liste de paires repr\u00e9sentant les \u00e9changes entre l'utilisateur et le bot.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#structure-de-la-memoire-de-conversation","title":"Structure de la M\u00e9moire de Conversation","text":"<p>La m\u00e9moire de conversation peut \u00eatre d\u00e9finie comme une liste ordonn\u00e9e de paires \\((u_i, d_i)\\), o\u00f9 \\(u_i\\) repr\u00e9sente l'entr\u00e9e utilisateur et \\(d_i\\) la r\u00e9ponse du bot pour le \\(i\\)-i\u00e8me \u00e9change. Cette liste est not\u00e9e \\(\\mathcal{C}\\) :</p> \\[ \\mathcal{C} = [(u_1, d_1), (u_2, d_2), \\ldots, (u_n, d_n)] \\] <p>o\u00f9 \\(n\\) est le nombre total d'\u00e9changes dans l'historique actuel.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#mise-a-jour-de-la-memoire","title":"Mise \u00e0 Jour de la M\u00e9moire","text":"<p>Lorsqu'un nouvel \u00e9change se produit, une nouvelle paire \\((u_{n+1}, d_{n+1})\\) est ajout\u00e9e \u00e0 la m\u00e9moire. Si la taille de \\(\\mathcal{C}\\) d\u00e9passe une limite maximale pr\u00e9d\u00e9finie \\(M_{\\text{max}}\\), l'\u00e9change le plus ancien est retir\u00e9 :</p> \\[ \\mathcal{C} =  \\begin{cases}  \\mathcal{C} \\cup \\{(u_{n+1}, d_{n+1})\\}, &amp; \\text{si } |\\mathcal{C}| &lt; M_{\\text{max}} \\\\ (\\mathcal{C} \\setminus \\{(u_1, d_1)\\}) \\cup \\{(u_{n+1}, d_{n+1})\\}, &amp; \\text{si } |\\mathcal{C}| = M_{\\text{max}} \\end{cases} \\]"},{"location":"blog/2024/08/05/chatbot-memory/#comptage-des-mots","title":"Comptage des Mots","text":"<p>Pour g\u00e9rer l'espace de m\u00e9moire et d\u00e9cider quand la compression est n\u00e9cessaire, nous calculons le nombre total de mots \\(W(\\mathcal{C})\\) dans la m\u00e9moire :</p> \\[ W(\\mathcal{C}) = \\sum_{(u_i, d_i) \\in \\mathcal{C}} (|u_i| + |d_i|) \\] <p>o\u00f9 \\(|u_i|\\) et \\(|d_i|\\) sont respectivement le nombre de mots dans \\(u_i\\) et \\(d_i\\).</p>"},{"location":"blog/2024/08/05/chatbot-memory/#compression-de-la-memoire","title":"Compression de la M\u00e9moire","text":"<p>Lorsque \\(W(\\mathcal{C})\\) d\u00e9passe un seuil \\(W_{\\text{max}}\\), la m\u00e9moire est compress\u00e9e pour maintenir la pertinence du contexte. Cette compression est r\u00e9alis\u00e9e par un mod\u00e8le de r\u00e9sum\u00e9 \\(\\mathcal{S}\\), tel que BART :</p> \\[ \\mathcal{C}_{\\text{compressed}} = \\mathcal{S}(\\mathcal{C}) \\] <p>o\u00f9 \\(\\mathcal{C}_{\\text{compressed}}\\) est la version r\u00e9sum\u00e9e de la m\u00e9moire, r\u00e9duisant le nombre total de mots tout en pr\u00e9servant l'essence des interactions pass\u00e9es.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#integration-dans-le-modele-de-langage","title":"Int\u00e9gration dans le Mod\u00e8le de Langage","text":"<p>Le mod\u00e8le de langage utilise le contexte compress\u00e9 pour g\u00e9n\u00e9rer des r\u00e9ponses pertinentes. Le prompt \\(P\\) utilis\u00e9 par le mod\u00e8le est construit comme suit :</p> \\[ P = f(\\mathcal{C}_{\\text{compressed}}, \\text{contexte}) \\] <p>o\u00f9 \\(\\text{contexte}\\) est le contexte suppl\u00e9mentaire r\u00e9cup\u00e9r\u00e9 \u00e0 partir d'un pipeline RAG, et \\(f\\) est une fonction de concat\u00e9nation qui pr\u00e9pare le texte pour le mod\u00e8le.</p> <p>Cette approche assure que le chatbot dispose toujours d'un contexte conversationnel \u00e0 jour, permettant des interactions plus naturelles et engageantes avec l'utilisateur.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#implementation-du-code-pour-la-gestion-de-la-memoire-dun-chatbot","title":"Impl\u00e9mentation du Code pour la Gestion de la M\u00e9moire d'un Chatbot","text":"<p>Dans cette section, nous allons examiner un exemple de code en Python qui illustre la gestion de la m\u00e9moire dans un chatbot. Le code utilise PyTorch et les transformers de Hugging Face pour g\u00e9rer et compresser l'historique des conversations.</p>"},{"location":"blog/2024/08/05/chatbot-memory/#preparation-de-lenvironnement","title":"Pr\u00e9paration de l'Environnement","text":"<p>Nous commen\u00e7ons par v\u00e9rifier si un GPU est disponible, ce qui permet d'acc\u00e9l\u00e9rer le traitement si n\u00e9cessaire.</p> <pre><code>import torch\nfrom transformers import pipeline\nimport logging\n\nif torch.cuda.is_available():\n    device: int = 0\nelse:\n    device: int = -1\n\nMAX_MEMORY_SIZE: int = 2000\n</code></pre>"},{"location":"blog/2024/08/05/chatbot-memory/#definition-de-la-classe-chatbotmemory","title":"D\u00e9finition de la Classe ChatbotMemory","text":"<p>La classe ChatbotMemory g\u00e8re l'historique des conversations et effectue des op\u00e9rations de mise \u00e0 jour et de compression. Donc \u00e0 chaques fois que update_memory est appel\u00e9, le texte en m\u00e9moire est compt\u00e9 et trait\u00e9 au besoin.</p> <pre><code>class ChatbotMemory:\n    def __init__(self, conv: list = []):\n        self.conversation_history = conv\n\n    def update_memory(self, user_input: str, bot_response: str) -&gt; None:\n        self.conversation_history.append(f\"'user': {user_input}, 'bot': {bot_response}\")\n\n        if memory_counter(self.conversation_history) &gt; 1000:\n            self.conversation_history = compressed_memory(self.conversation_history)\n            logging.info(\"M\u00e9moire compress\u00e9e.\")\n\n        if len(self.conversation_history) &gt; MAX_MEMORY_SIZE:\n            self.conversation_history.pop(0)\n            logging.info(\"M\u00e9moire r\u00e9duite.\")\n        return 0\n\n    def get_memory(self):\n        return self.conversation_history\n</code></pre>"},{"location":"blog/2024/08/05/chatbot-memory/#compression-et-comptage-de-la-memoire","title":"Compression et Comptage de la M\u00e9moire","text":"<p>La fonction _get_compressed_memory utilise le mod\u00e8le BART pour r\u00e9sumer l'historique des conversations.</p> <pre><code>def _get_compressed_memory(sentence: str) -&gt; str:\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n    summary = summarizer(sentence, max_length=50, min_length=5, do_sample=False)\n    return summary[0]['summary_text']\n</code></pre> <p>La fonction compressed_memory applique la fonction _get_compressed_memory \u00e0 chaque segment de l'historique des conversations. Pour cela nous optimisons la proc\u00e9dure en effectuant un traitement par Batch. Cette m\u00e9thode est dissoci\u00e9 de la fonction _get_compressed_memory de mani\u00e8re \u00e0 pouvoir introduire de nouvelles m\u00e9thodes de compression.</p> <pre><code>def compressed_memory(conv_hist: list) -&gt; list:\n    return [_get_compressed_memory(' '.join(conv_hist[i:i+5])) for i in range(0, len(conv_hist), 5)]\n</code></pre> <p>La fonction memory_counter compte le nombre total de mots dans l'historique. (Note qu'il pourrais \u00eatre interessant de r\u00e9aliser cette \u00e9tape avec des tokens plutot que des mots.)</p> <pre><code>def memory_counter(conv_hist: list) -&gt; int:\n    st = ''.join(conv_hist)\n    return len(st.split())\n</code></pre>"},{"location":"blog/2024/08/05/chatbot-memory/#conclusion","title":"Conclusion","text":"<p>Ce code \u00e9tablit un cadre efficace pour la gestion de la m\u00e9moire dans un chatbot, en utilisant des techniques de compression pour maintenir un contexte pertinent et en am\u00e9liorant la performance globale du syst\u00e8me. L'utilisation de mod\u00e8les de r\u00e9sum\u00e9 comme BART assure que m\u00eame lorsque la m\u00e9moire est compress\u00e9e, le contexte essentiel est pr\u00e9serv\u00e9.</p> <p>Code sur GitHub</p>"},{"location":"blog/2024/06/21/s%C3%A9curit%C3%A9-de-vos-codes/","title":"S\u00e9curit\u00e9 de vos codes","text":"<p>La s\u00e9curit\u00e9 avec Bandit, un outil d'analyse de s\u00e9curit\u00e9 de code pour Python.</p> <p>\ud83d\udd0d Pourquoi Bandit ?</p> <p>Analyse automatique : Bandit passe en revue votre code pour d\u00e9tecter les usages courants qui peuvent \u00eatre dangereux pour la s\u00e9curit\u00e9.</p> <p>Int\u00e9gration facile : S'int\u00e8gre dans vos pipelines CI/CD pour des contr\u00f4les de s\u00e9curit\u00e9 continus.</p> <p>Mettre en place Bandit est aussi simple que:</p> <pre><code>\u200bpip install bandit\nbandit -r your_code.py\u200b\n</code></pre> <p>Ajouter \u00e0 un pipeline GitHub Action, \u00e0 la suite des tests unitaires:</p> <pre><code>\u200bname: Security check\u200b\n\u200brun: bandit -r your_code.py\n</code></pre> <p>Vous pouvez aussi l'int\u00e9grer dans un nouveau workflows en ajoutant un nouveau fichier .yml.</p> <p>Jetez un \u0153il \u00e0 l'image jointe pour voir Bandit en action dans un workflow GitHub Actions. Aucune vuln\u00e9rabilit\u00e9 trouv\u00e9e, c'est exactement ce que nous voulons voir !</p> <p></p> <p>Avec Bandit, votre code est surveill\u00e9 pour \u00e9viter les erreurs courantes qui pourraient le compromettre et ce \u00e0 chaque push ! C'est super pratique et cela doit devenir une habitude.</p>"},{"location":"blog/2024/04/21/la-documentation-technique-et-sphinx/","title":"La documentation technique et Sphinx","text":"<p>Ici, je ne vous parlerai pas du README (qui est un guide rapide), mais plut\u00f4t de la documentation technique.</p> <p>Une documentation claire et accessible est aussi cruciale que le code lui-m\u00eame. Elle guide les utilisateurs et les contributeurs, facilitant l'utilisation et la contribution au projet.</p> <p>Voici comment structurer et publier une documentation d'un projet Python, en utilisant Sphinx pour la g\u00e9n\u00e9ration et Read the Docs pour l'h\u00e9bergement.</p> <ol> <li>Structuration du Projet \ud83d\udcc2 </li> </ol> <p> La premi\u00e8re \u00e9tape est d'organiser le projet en suivant les meilleures pratiques, avec des dossiers distincts pour les sources (src/), les tests (tests/), la documentation (docs/) et le script setup.py. Cette structure claire facilite la navigation et la maintenance du projet. (Je ferai un post sur toute cette partie.) </p> <ol> <li>G\u00e9n\u00e9ration de Documentation avec Sphinx \ud83d\udcd6</li> </ol> <p>Sphinx transforme les docstrings en une documentation compl\u00e8te et bien format\u00e9e.</p> <p>Nous utilisons des extensions comme autodoc pour g\u00e9n\u00e9rer automatiquement la documentation \u00e0 partir des docstrings.</p> <p>1/ Nous installons Sphinx :</p> <pre><code>pip install sphinx\n</code></pre> <p>2/ Nous cr\u00e9ons les dossiers d'initialisation :</p> <pre><code>sphinx-quickstart docs\n</code></pre> <p>3/ Nous nous d\u00e9pla\u00e7ons dans docs et nous cr\u00e9ons la documentation :</p> <pre><code>cd docs\nmake html\n</code></pre> <p>Nous pouvons supprimer la documentation avec :  <code>zsh make clean</code></p> <ol> <li>Configuration pour Read the Docs \ud83c\udf10</li> </ol> <p> Avec un fichier .readthedocs.yaml, nous configurons le processus de build sur Read the Docs, en sp\u00e9cifiant la version de Python et les d\u00e9pendances n\u00e9cessaires. Une fois cela fait, rendez-vous sur ReadTheDoc et s\u00e9lectionnez le repo contenant votre projet. Cela assure que la documentation est automatiquement mise \u00e0 jour et accessible en ligne \u00e0 chaque commit. (Vous trouverez un exemple dans l'image de ce post.) </p> <p>Le r\u00e9sultat ? Une documentation en ligne toujours \u00e0 jour, facilement accessible par les utilisateurs et les contributeurs.</p> <p> La documentation ne doit jamais \u00eatre une r\u00e9flexion apr\u00e8s coup dans le d\u00e9veloppement logiciel. Elle est essentielle pour la transparence, l'accessibilit\u00e9 et la r\u00e9ussite \u00e0 long terme d'un projet. <p>"},{"location":"blog/2024/06/21/automatisation-des-tests/","title":"Automatisation des Tests","text":"<p>Vous le savez (enfin j'esp\u00e8re), les tests sont cruciaux pour tous les projets de d\u00e9veloppement. Mais les ex\u00e9cuter manuellement \u00e0 chaque fois ? Pas tr\u00e8s 2024 ! \ud83e\udd16</p> <p>Voici une astuce pour les d\u00e9veloppeurs soucieux d'efficacit\u00e9 : l'int\u00e9gration de Pytest avec GitHub Actions.</p> <p>Pytest est un framework de test pour vos applications Python. Et quand vous combinez cela avec la puissance des GitHub Actions, vous obtenez une suite de tests automatis\u00e9s qui s'ex\u00e9cutent \u00e0 chaque push ou pull request, assurant que vos modifications n'introduisent pas de r\u00e9gressions !</p> <p>Pour configurer une action de base GitHub et ex\u00e9cuter vos tests Pytest, cr\u00e9ez un fichier <code>.github/workflows/python-app.yml</code>\u200b dans votre repo et ajouter votre configuration comme sur mon exemple en photo.</p> <p></p> <p>\ud83d\ude80 Et voil\u00e0 ! Vos tests se lancent automatiquement, vous offrant une tranquillit\u00e9 d'esprit.</p> <p>Je suis persuad\u00e9 que certains d'entre vous utilisent d'autres techniques ou des outils plus performants pour les tests, l'essentiel \u00e9tant de toujours pr\u00e9venir son code de toutes formes de r\u00e9gression !</p>"},{"location":"projets/","title":"Projets","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/chatbot/","title":"Chatbot","text":""},{"location":"blog/category/artificial-intelligence/","title":"Artificial Intelligence","text":""},{"location":"blog/category/mathematics/","title":"Mathematics","text":""},{"location":"blog/category/nlp/","title":"NLP","text":""},{"location":"blog/category/machine-learning/","title":"Machine Learning","text":""},{"location":"blog/category/memory/","title":"Memory","text":""},{"location":"blog/category/devops/","title":"DevOps","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/testing/","title":"Testing","text":""}]}